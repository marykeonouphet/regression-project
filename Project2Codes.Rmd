---
title: "Project2Codes"
output: pdf_document
---
## Import data
```{r import}
proj2.data <- read.csv("saratoga.csv", colClasses=c(rep("numeric", 10), rep("factor",6)))
head(proj2.data)
summary(proj2.data)
length(proj2.data$price)
names(proj2.data)
dim(proj2.data)
```

## Odd observations
```{r odds}
proj2.data[which(proj2.data$sewer=="none"),]
proj2.data[which(proj2.data$bathrooms<1),]
proj2.data[which(proj2.data$lotSize==0),]
proj2.data[which(proj2.data$landValue > proj2.data$price),]
```
### obs 851 and 891 data looks exactly the same except they have different number of rooms
### obs 851, 891, and 1011 have landvalue > price
### obs 16 and 702 have lot size 0
### obs 1494 has 0 bathrooms
### obs 10, 18, 37, 40, 46, 91, 134, 498, 564, 1018, 1181, and 1254 has no sewer


## Full linear model
```{r full linear model}
myfullmodel<-lm(price~lotSize+age+landValue+livingArea+pctCollege+bedrooms+fireplaces+bathrooms+rooms+heating+fuel+sewer+waterfront+newConstruction+centralAir, data=proj2.data)
summary(myfullmodel)
```
### pctcollege, fireplaces, heating, fuel, and sewer are insignificant regressors based on their t statistics and their corresponding p-values. Adjusted r squared is reasonable for a large data set with multiple regressors.


## Checking model adequacy
```{r model adequacy}
qqnorm(rstudent(myfullmodel), main="Normal QQ Plot Full Model")
abline(0,1,col="red")

t<- rstudent(myfullmodel)
plot(fitted.values(myfullmodel), t, 
     xlab="fitted values", 
     ylab="studentized residuals",
     main="Residual Plot Full Model")
abline(0,0,col="red")

n <- length(proj2.data$price)
k <- 15
plot(hatvalues(myfullmodel),rstudent(myfullmodel),
     xlab="hat values", ylab="studentized residuals", 
     pch=16, main="Residual vs Leverage Full Model", 
     cex=2, cex.main=2, cex.lab=1.5, cex.axis=1.5)
abline(v=2*(k+1)/n, col="red",lwd=3, lty=2)
abline(h=-3, col="red",lwd=3, lty=2)
abline(h=3, col="red",lwd=3, lty=2)

plot(cooks.distance(myfullmodel), xlab="index", ylab="D_i", 
     pch=16, main="Cook's Distance Plot Full Model", 
     cex=2, cex.main=2, cex.lab=1.5, cex.axis=1.5)
abline(h=1, col="red",lwd=3, lty=2)

plot(dffits(myfullmodel), xlab="index", ylab="dffits", 
     pch=16, main="DFFITS Full Model", 
     cex=2, cex.main=3, cex.lab=2.5, cex.axis=2.5)
abline(h=2*sqrt((k+1)/n), col="red",lwd=3, lty=2)
abline(h=-2*sqrt((k+1)/n), col="red",lwd=3, lty=2)

plot(covratio(myfullmodel), xlab="index", ylab="covratio",
     pch=16, main="COVRATIO Full Model", 
     cex=2, cex.main=3, cex.lab=2.5, cex.axis=2.5)
abline(h=1+3*(k+1)/n, col="red",lwd=3, lty=2)
abline(h=1-3*(k+1)/n, col="red",lwd=3, lty=2)

plot(dfbetas(myfullmodel)[,1], xlab="index", ylab="dfbeta_0", 
     pch=16, main="DFBETAS_0", 
     cex=2, cex.main=2, cex.lab=1.5, cex.axis=1.5)
abline(h=2/sqrt(n), col="red",lwd=3, lty=2)
abline(h=-2/sqrt(n), col="red",lwd=3, lty=2)

## obs 1254
proj2.data[which(dfbetas(myfullmodel)[,1] > 0.8),]

plot(dfbetas(myfullmodel)[,8], xlab="index", ylab="dfbeta_7", 
     pch=16, main="DFBETAS_7", 
     cex=2, cex.main=2, cex.lab=1.5, cex.axis=1.5)
abline(h=2/sqrt(n), col="red",lwd=3, lty=2)
abline(h=-2/sqrt(n), col="red",lwd=3, lty=2)

# obs 1202
proj2.data[which(dfbetas(myfullmodel)[,8] > 0.6),]

plot(dfbetas(myfullmodel)[,15], xlab="index", ylab="dfbeta_14", 
     pch=16, main="DFBETAS_14", 
     cex=2, cex.main=2, cex.lab=1.5, cex.axis=1.5)
abline(h=2/sqrt(n), col="red",lwd=3, lty=2)
abline(h=-2/sqrt(n), col="red",lwd=3, lty=2)

# obs 1254
proj2.data[which(dfbetas(myfullmodel)[,15] < -1),]

plot(dfbetas(myfullmodel)[,16], xlab="index", ylab="dfbeta_15", 
     pch=16, main="DFBETAS_15", 
     cex=2, cex.main=2, cex.lab=1.5, cex.axis=1.5)
abline(h=2/sqrt(n), col="red",lwd=3, lty=2)
abline(h=-2/sqrt(n), col="red",lwd=3, lty=2)

# obs 1254
proj2.data[which(dfbetas(myfullmodel)[,16] < -1),]

# obs 1254 and 1202
proj2.data[which(hatvalues(myfullmodel) > 2*(k+1)/n & abs(rstudent(myfullmodel)) > 3),]

plot(myfullmodel)
```
### There is an issue with nonnormality. Residual plot against fitted values looks good. No issue with nonconstant variance or nonlinearity. There are some outliers with values above the absolute value of 3. The residual vs. leverage plot shows that there are some influential points with high leverage and large residuals in the data set. The cook's distance plot shows that there are no influential points. The dffits plot shows that there are many points that have influence on their fitted values.The covratio plot shows that there are many points that have positive influence on the precision of coefficient estimates, while there are many points that have negative influence on the precision of coefficient estimates. The dfbeta plots show that observation 1254 has some influence on the intercept and regressors 14 and 15. The dfbeta plots also show that observation 1202 has influence on regressor 7. These observations should be looked at since they also appear as influential points in the leverage vs. residual plot.


## Transformations
```{r transformations} 
### log transformations ####
proj2.data$price2 = log(proj2.data$price)

logtransformmodel <- lm(price2~lotSize+age+landValue+livingArea+pctCollege+bedrooms+fireplaces+bathrooms+rooms+heating+fuel+sewer+waterfront+newConstruction+centralAir, data=proj2.data)
summary(logtransformmodel)

# plot studentized residuals against fitted values
plot(logtransformmodel$fitted.values, rstudent(logtransformmodel), 
     xlab="fitted values", ylab="studentized residuals", 
     pch=16, main="residual plot (log transformation)", 
     cex=1.5, cex.main=2, cex.lab=1.5, cex.axis=1.5)
abline(0, 0, col="red",lwd=3) # add the regression line to plot

qqnorm(rstudent(logtransformmodel),main="QQ plot (log transformation)")
abline(0,1,col="red")


### square root transformation ###
proj2.data$price3 = sqrt(proj2.data$price)

sqrttransformmodel <- lm(price3~lotSize+age+landValue+livingArea+pctCollege+bedrooms+fireplaces+bathrooms+rooms+heating+fuel+sewer+waterfront+newConstruction+centralAir, data=proj2.data)
summary(sqrttransformmodel)

# plot studentized residuals against fitted values
plot(sqrttransformmodel$fitted.values, rstudent(sqrttransformmodel), 
     xlab="fitted values", ylab="studentized residuals", 
     pch=16, main="residual plot (sqrt transformation)", 
     cex=1.5, cex.main=2, cex.lab=1.5, cex.axis=1.5)
abline(0, 0, col="red",lwd=3) # add the regression line to plot

qqnorm(rstudent(sqrttransformmodel),main="QQ plot (sqrt transformation)")
abline(0,1,col="red")


### using boxcox lambda transformation ###
proj2.data$price4 = (proj2.data$price)^(1/4)

boxcoxtransformmodel <- lm(price4~lotSize+age+landValue+livingArea+pctCollege+bedrooms+fireplaces+bathrooms+rooms+heating+fuel+sewer+waterfront+newConstruction+centralAir, data=proj2.data)
summary(boxcoxtransformmodel)

# plot studentized residuals against fitted values
plot(boxcoxtransformmodel$fitted.values, rstudent(boxcoxtransformmodel), 
     xlab="fitted values", ylab="studentized residuals", 
     pch=16, main="residual plot (boxcox transformation)", 
     cex=1.5, cex.main=2, cex.lab=1.5, cex.axis=1.5)
abline(0, 0, col="red",lwd=3) # add the regression line to plot

qqnorm(rstudent(boxcoxtransformmodel),main="QQ plot (boxcox transformation)")
abline(0,1,col="red")
```
### Adj R squared original model: 0.6498
### Adj R squared log model: 0.5787
### Adj R squared sqrt model: 0.6453
### Adj R squared boxcox model: 0.6231


## BoxCox
```{r boxcox}
library(MASS)
par(cex=1, cex.main=2, cex.lab=1.5, cex.axis=1.5)
boxcox(price~lotSize+age+landValue+livingArea+pctCollege+bedrooms+fireplaces+bathrooms+rooms+heating+fuel+sewer+waterfront+newConstruction+centralAir, data=proj2.data)
```
### the boxcox plot shows that the optimal lambda to transform the response would be 0.25.


## Compare QQplots and residual plots
```{r compare qqplots and residual plots}
qqnorm(rstudent(myfullmodel))
abline(0,1,col="red")

qqnorm(rstudent(logtransformmodel),main="QQ plot (log transformation)")
abline(0,1,col="red")

qqnorm(rstudent(sqrttransformmodel),main="QQ plot (sqrt transformation)")
abline(0,1,col="red")

qqnorm(rstudent(boxcoxtransformmodel),main="QQ plot (boxcox transformation)")
abline(0,1,col="red")

plot(myfullmodel$fitted.values, rstudent(myfullmodel), 
     xlab="fitted values", ylab="studentized residuals", 
     pch=16, main="residual plot", 
     cex=1.5, cex.main=2, cex.lab=1.5, cex.axis=1.5)
abline(0, 0, col="red",lwd=3)

plot(logtransformmodel$fitted.values, rstudent(logtransformmodel), 
     xlab="fitted values", ylab="studentized residuals", 
     pch=16, main="residual plot (log transformation)", 
     cex=1.5, cex.main=2, cex.lab=1.5, cex.axis=1.5)
abline(0, 0, col="red",lwd=3)

plot(sqrttransformmodel$fitted.values, rstudent(sqrttransformmodel), 
     xlab="fitted values", ylab="studentized residuals", 
     pch=16, main="residual plot (sqrt transformation)", 
     cex=1.5, cex.main=2, cex.lab=1.5, cex.axis=1.5)
abline(0, 0, col="red",lwd=3)

plot(boxcoxtransformmodel$fitted.values, rstudent(boxcoxtransformmodel), 
     xlab="fitted values", ylab="studentized residuals", 
     pch=16, main="residual plot (boxcox transformation)", 
     cex=1.5, cex.main=2, cex.lab=1.5, cex.axis=1.5)
abline(0, 0, col="red",lwd=3)
```
### The log transformation of the response is worse than the original model. Both the square root and boxcox transformations are better than the original model. The QQplot looks better in the square root and boxcox tranformations than the original model. All 3 residual plots (not log transformation) appear adequate as far as constant variance and linearity, but the square root transformation seems to do a better job of reducing large residuals(scale of -6,6). Overall, I would conclude that the square root transformation is the best model as far as the qqplot and residual plot.


```{r indicator variables}
proj2.data$waterfront0 <- ifelse(proj2.data$waterfront=="Yes", 0,1)
proj2.data$newConstruction0 <- ifelse(proj2.data$newConstruction=="Yes",0,1)
proj2.data$centralAir0 <- ifelse(proj2.data$centralAir=="Yes",0,1)

heating1 <- ifelse(proj2.data$heating=="hot air",1,0)
heating2 <- ifelse(proj2.data$heating=="hot water/steam",1,0)
proj2.data$heating0 <- matrix(c(heating1,heating2),length(heating1),2)

fuel1 <- ifelse(proj2.data$fuel=="gas",1,0)
fuel2 <- ifelse(proj2.data$fuel=="oil",1,0)
proj2.data$fuel0 <- matrix(c(fuel1,fuel2),length(fuel1),2)

sewer1 <- ifelse(proj2.data$sewer=="public/commercial",1,0)
sewer2 <- ifelse(proj2.data$sewer=="septic",1,0)
proj2.data$sewer0 <- matrix(c(sewer1,sewer2),length(sewer1),2)

summary(proj2.data)
names(proj2.data)
```


## Multicollinearity of original model
```{r collineairty}
remove <- c(1,11:19)

scaled.data <- scale(proj2.data[,-remove])
C <- cor(scaled.data)
C

diag(solve(C))

X <- as.matrix(scaled.data)
kappa(t(X) %*% X, exact=TRUE)
```
### rooms-livingarea: 0.7336658
### bathrooms-livingarea: 0.7185637
### bedrooms-livingarea: 0.6561957
### rooms-bedrooms: 0.6718633
### rooms-bathrooms: 0.51758469

### High vifs for heating1, heating2, fuel1, fuel2, sewer1, sewer2.
### 254.7621 is a moderate condition number

### Overall, there is moderate multicollinearity in the full model. This can possibly be fixed by variable selection and eliminating regressors that are redundant and insignificant.


## Variable selection
```{r variable selection}
# AIC-based backward elimination 
fullModel<-lm(price~lotSize+age+landValue+livingArea+pctCollege+bedrooms+fireplaces+bathrooms+rooms+heating+fuel+sewer+waterfront+newConstruction+centralAir, data=proj2.data)
step(fullModel, direction="backward", k=2)

fullModel2<-lm(price3~lotSize+age+landValue+livingArea+pctCollege+bedrooms+fireplaces+bathrooms+rooms+heating+fuel+sewer+waterfront+newConstruction+centralAir, data=proj2.data)
step(fullModel2, direction="backward", k=2)

fullModel3<-lm(price4~lotSize+age+landValue+livingArea+pctCollege+bedrooms+fireplaces+bathrooms+rooms+heating+fuel+sewer+waterfront+newConstruction+centralAir, data=proj2.data)
step(fullModel3, direction="backward", k=2)

```
### backward elimination of original model gives predictors: lotsize, age, landvalue, livingarea, bedrooms, bathrooms, rooms, heating, waterfront, newconstruction, and central air.

### backward elimination of square root model gives predictors: lotSize, age, landValue, livingArea, bathrooms, rooms, heating, waterfront, newConstruction, centralAir

### backward elimination of boxcox model gives predictors: lotSize, age, landValue, livingArea, bathrooms, rooms, heating, waterfront, newConstruction, centralAir


## Compare reduced models
```{r compare models}
summary(myfullmodel)

reducedoriginalmodel<-lm(price~lotSize+age+landValue+livingArea+bedrooms+bathrooms+rooms+heating+waterfront+newConstruction+centralAir, data=proj2.data)
summary(reducedoriginalmodel)

reducedsqrtmodel<-lm(price3~lotSize+age+landValue+livingArea+bathrooms+rooms+heating+waterfront+newConstruction+centralAir, data=proj2.data)
summary(reducedsqrtmodel)

reducedboxcoxmodel<-lm(price4~lotSize+age+landValue+livingArea+bathrooms+rooms+heating+waterfront+newConstruction+centralAir, data=proj2.data)
summary(reducedsqrtmodel)
```
### original model: adj r sq = 0.6498 --- msres = (58260)^2
### reduced original model: adj r sq = 0.6506 --- msres = (58190)^2
### reduced sq rt model: adj r sq = 0.646 --- msres = (59.62)^2
### reduced boxcox model: adj r sq = 0.646 --- msres = (59.62)^2


## residual plots for reduced models
```{r residual plots for reduced models}
qqnorm(rstudent(reducedoriginalmodel),main="QQ plot (reduced original model)")
abline(0,1,col="red")

qqnorm(rstudent(reducedsqrtmodel),main="QQ plot (reduced sqrt model)")
abline(0,1,col="red")

qqnorm(rstudent(reducedboxcoxmodel),main="QQ plot (reduced boxcox model)")
abline(0,1,col="red")

plot(reducedoriginalmodel$fitted.values, rstudent(reducedoriginalmodel), 
     xlab="fitted values", ylab="studentized residuals", 
     pch=16, main="residual plot (reduced original model)", 
     cex=1.5, cex.main=2, cex.lab=1.5, cex.axis=1.5)
abline(0, 0, col="red",lwd=3)

plot(reducedsqrtmodel$fitted.values, rstudent(reducedsqrtmodel), 
     xlab="fitted values", ylab="studentized residuals", 
     pch=16, main="residual plot (reduced sqrt model)", 
     cex=1.5, cex.main=2, cex.lab=1.5, cex.axis=1.5)
abline(0, 0, col="red",lwd=3)

plot(reducedboxcoxmodel$fitted.values, rstudent(reducedboxcoxmodel), 
     xlab="fitted values", ylab="studentized residuals", 
     pch=16, main="residual plot (reduced boxcox model)", 
     cex=1.5, cex.main=2, cex.lab=1.5, cex.axis=1.5)
abline(0, 0, col="red",lwd=3)
```
### the reduced sqrt model has the best QQplot and residual plot.


## multicollineaity reduced models
```{r multicollinearity reduced models}
## reduced original model
remove2 <- c(1,11:19,6,8,24,25)

scaled.data2 <- scale(proj2.data[,-remove2])
C2 <- cor(scaled.data2)
C2

diag(solve(C2))

X2 <- as.matrix(scaled.data2)
kappa(t(X2) %*% X2, exact=TRUE)


## reduced sqrt and boxcox model
remove3 <- c(1,11:19,6,8,24,25,7)

scaled.data3 <- scale(proj2.data[,-remove3])
C3 <- cor(scaled.data2)
C3

diag(solve(C3))

X3 <- as.matrix(scaled.data3)
kappa(t(X3) %*% X3, exact=TRUE)
```
### reduced original model: there are still some moderate pairwise correlations between rooms, living area, bathrooms, and bedrooms as expected, but all scores averaging around 0.5-0.7(moderate correlation). There is also a pairwise correlation between heating 1 and heating 2 since they are different levels of the same category. Overall, all predictors have small VIFs indicating that there is no serious issue with multicollinearity. The small condition number of 18.8641 also indicates that there is no serious issue with multicollinearity.

### reduced sqrt and boxcox models: same as the reduced original model. Condition number, 16.45352.

### multicollinearity issue was fixed by eliminating redundant and insignificant predictors from the model.



## Conclusion
```{r final model}
summary(reducedsqrtmodel)

## qqnorm
qqnorm(rstudent(reducedsqrtmodel),main="QQ plot Final Model")
abline(0,1,col="red")

## residual plots
plot(reducedsqrtmodel$fitted.values, rstudent(reducedsqrtmodel), 
     xlab="fitted values", ylab="studentized residuals", 
     pch=16, main="Residual Plot Final Model", 
     cex=1.5, cex.main=2, cex.lab=1.5, cex.axis=1.5)
abline(0, 0, col="red",lwd=3)

n <- length(proj2.data$price3)
k <- 10
plot(hatvalues(reducedsqrtmodel),rstudent(reducedsqrtmodel),
     xlab="hat values", ylab="studentized residuals", 
     pch=16, main="Residual vs leverage Final Model", 
     cex=2, cex.main=2, cex.lab=1.5, cex.axis=1.5)
abline(v=2*(k+1)/n, col="red",lwd=3, lty=2)
abline(h=-3, col="red",lwd=3, lty=2)
abline(h=3, col="red",lwd=3, lty=2)

plot(cooks.distance(reducedsqrtmodel), xlab="index", ylab="D_i", 
     pch=16, main="Cook's distance Final Model", 
     cex=2, cex.main=2, cex.lab=1.5, cex.axis=1.5)
abline(h=1, col="red",lwd=3, lty=2)

plot(dffits(reducedsqrtmodel), xlab="index", ylab="dffits", 
     pch=16, main="DFFITS Final Model", 
     cex=2, cex.main=3, cex.lab=2.5, cex.axis=2.5)
abline(h=2*sqrt((k+1)/n), col="red",lwd=3, lty=2)
abline(h=-2*sqrt((k+1)/n), col="red",lwd=3, lty=2)

plot(covratio(reducedsqrtmodel), xlab="index", ylab="covratio",
     pch=16, main="COVRATIO Final Model", 
     cex=2, cex.main=3, cex.lab=2.5, cex.axis=2.5)
abline(h=1+3*(k+1)/n, col="red",lwd=3, lty=2)
abline(h=1-3*(k+1)/n, col="red",lwd=3, lty=2)

## multicollinearity
remove3 <- c(1,11:19,6,8,24,25,7)

scaled.data3 <- scale(proj2.data[,-remove3])
C3 <- cor(scaled.data2)
C3

diag(solve(C3))

X3 <- as.matrix(scaled.data3)
kappa(t(X3) %*% X3, exact=TRUE)
```
### I can conclude that the best model is the reduced square root model, where the response is transformed by taking its square root. This helps reduce some of the large residuals and the normality issue. Reducing the number of predictors helped with the multicollinearity issue. There is no issue with nonconstant variance or nonlinearity. This model is significant with a p-value of <2.2e-16 and F statistic 287.5. The adjusted r squared, 0.646, is adequate for a large data set and MSres, 59.62, is much smaller than the original model.

## The best linear regression model:

## sqrt(price) = 229.5 + 8.280lotSize - 0.2287age + 0.0008674landValue + 0.06698livingArea + 24.72bathrooms + 2.171rooms + 13.08heatinghotair + 4.516heatinghotwatersteam + 115.8waterfrontYes - 41.30newConstructionYes + 10.77centralAirYes


